{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_dim\n",
    "        ), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.proj_out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, D = x.size()  # Batch size, sequence length, embedding dimension\n",
    "\n",
    "        # Project queries, keys, and values\n",
    "        qkv = self.qkv_proj(x)\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "\n",
    "        # Reshape q, k, v to have multiple heads\n",
    "        q = q.view(B, L, self.num_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )  # B x num_heads x L x head_dim\n",
    "        k = k.view(B, L, self.num_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )  # B x num_heads x L x head_dim\n",
    "        v = v.view(B, L, self.num_heads, self.head_dim).transpose(\n",
    "            1, 2\n",
    "        )  # B x num_heads x L x head_dim\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (\n",
    "            self.head_dim**0.5\n",
    "        )  # B x num_heads x L x L\n",
    "\n",
    "        # Apply attention mask (if needed)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # B x num_heads x L x L\n",
    "\n",
    "        # Apply dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        attn_output = torch.matmul(attn_weights, v)  # B x num_heads x L x head_dim\n",
    "\n",
    "        # Concatenate heads and project out\n",
    "        attn_output = (\n",
    "            attn_output.transpose(1, 2).contiguous().view(B, L, self.embed_dim)\n",
    "        )  # B x L x embed_dim\n",
    "        attn_output = self.proj_out(attn_output)  # B x L x embed_dim\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, einsum\n",
    "from typing import Optional\n",
    "from torch import Tensor\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nheads, bias=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % nheads == 0, \"d_model must be divisible by nheads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.nheads = nheads\n",
    "        self.d_h = d_model // nheads\n",
    "        self.scaling = self.d_h ** -0.5\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_bias: Optional[Tensor] = None, mask=None, need_weights=False):\n",
    "\n",
    "        q = rearrange(self.wq(x), 'b n (h d) -> b h n d', h=self.nheads)\n",
    "        k = rearrange(self.wk(x), 'b n (h d) -> b h n d', h=self.nheads)\n",
    "        v = rearrange(self.wv(x), 'b n (h d) -> b h n d', h=self.nheads)\n",
    "        # Scaled Dot Product Attention\n",
    "        attn_weights = einsum( q, k, \"b h q d, b h k d -> b h q k\") * self.scaling\n",
    "        if attn_bias is not None:\n",
    "            attn_weights += rearrange(attn_bias, \"b i j -> b () i j\")\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = einsum( attn_weights, v, \"b n i j, b h j d -> b h i d\")\n",
    "        attn_output = rearrange(attn_output, \"b h n d -> b n (h d)\")\n",
    "        attn_output = self.wo(attn_output)\n",
    "        if need_weights:\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output\n",
    "\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, qheads, kvheads, bias=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % qheads == 0, \"d_model must be divisible by nheads\"\n",
    "        assert qheads % kvheads == 0, \"qheads must be divisible by kvheads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.qheads = qheads\n",
    "        self.kvheads = kvheads\n",
    "\n",
    "        self.d_q = d_model // qheads\n",
    "        d_kv = d_model // qheads * kvheads\n",
    "        self.scaling = self.d_q**-0.5\n",
    "        self.num_head_groups = qheads // kvheads\n",
    "\n",
    "        self.wq = nn.Linear(d_model, d_model, bias)\n",
    "        self.wk = nn.Linear(d_model, d_kv, bias)\n",
    "        self.wv = nn.Linear(d_model, d_kv, bias)\n",
    "        self.wo = nn.Linear(d_model, d_model, bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_bias: Optional[Tensor] = None, mask=None, need_weights=False):\n",
    "\n",
    "        q = rearrange(self.wq(x), \"b n (h d) -> b h n d\", h=self.qheads)\n",
    "        k = rearrange(self.wk(x), \"b s (h d) -> b h s d\", h=self.kvheads)\n",
    "        v = rearrange(self.wv(x), \"b s (h d) -> b h s d\", h=self.kvheads)\n",
    "        # Grouped Query Attention\n",
    "        q = rearrange(q, \"b (h g) n d -> b g h n d\", g=self.num_head_groups)\n",
    "        attn_weights = einsum(q, k, \"b g h n d, b h s d -> b g h n s\") * self.scaling\n",
    "        if attn_bias is not None:\n",
    "            attn_weights += rearrange(attn_bias, \"b i j -> b () () i j\")\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = einsum(attn_weights, v, \"b g h n s, b h s d -> b g h n d\")\n",
    "        attn_output = rearrange(attn_output, \"b g h n d -> b n (h g d)\")\n",
    "        attn_output = self.wo(attn_output)\n",
    "        if need_weights:\n",
    "            attn_weights = rearrange(attn_weights, \"b g h n s -> b n s (h g)\")\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "seq_length = 10\n",
    "embed_dim = 64\n",
    "num_heads = 8\n",
    "\n",
    "# Create input tensor\n",
    "input_tensor = torch.randn(batch_size, seq_length, embed_dim)\n",
    "\n",
    "# Initialize Multihead Attention layer\n",
    "multihead_attn = GroupedQueryAttention(embed_dim, 8, 4)\n",
    "\n",
    "# Forward pass\n",
    "output_tensor = multihead_attn(input_tensor)\n",
    "\n",
    "# Print output shape\n",
    "# print(\"Output shape:\", output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2270e-02, -1.1735e-01, -6.7048e-02, -2.4043e-01, -8.7736e-02,\n",
       "         -4.5804e-02, -1.9562e-02, -2.4857e-01,  1.4149e-01,  3.9893e-02,\n",
       "          1.3840e-01, -2.0862e-02,  7.3982e-02,  7.8651e-02,  5.7386e-02,\n",
       "          6.3551e-02, -3.4428e-02, -7.2068e-02,  1.3942e-01, -9.8994e-02,\n",
       "         -2.1609e-01, -2.1194e-01,  9.7416e-02, -8.3808e-02,  5.5601e-02,\n",
       "         -1.3729e-01,  1.3815e-01, -6.4847e-02,  1.0566e-01, -2.4299e-02,\n",
       "         -6.5047e-02, -1.8546e-01, -1.1985e-01, -3.0995e-01, -6.6507e-02,\n",
       "          8.6111e-02,  5.8910e-02, -5.2422e-02, -2.6515e-02, -3.3529e-02,\n",
       "          3.0535e-02, -6.8945e-02, -1.8473e-01,  7.3203e-03,  5.1725e-03,\n",
       "          2.7353e-01, -8.3754e-02,  2.5106e-02,  2.5990e-01,  2.7832e-02,\n",
       "         -9.1695e-03,  2.1049e-01,  2.3586e-01, -1.2316e-01,  8.9196e-02,\n",
       "          2.8696e-02, -1.4621e-01,  2.8020e-01,  9.1085e-02, -1.4379e-02,\n",
       "          1.7279e-01, -4.9902e-02, -1.3725e-01,  9.8382e-02],\n",
       "        [ 9.2150e-03, -1.5714e-01,  2.9604e-03, -2.4203e-01, -4.4704e-02,\n",
       "          5.3149e-02,  4.1661e-02, -2.2723e-01,  1.7062e-01,  9.2974e-02,\n",
       "          1.3801e-01, -5.5505e-02,  1.2011e-01,  1.0312e-01,  1.4254e-01,\n",
       "          1.2385e-01, -9.3999e-02, -1.4893e-01,  1.6248e-01, -1.1254e-01,\n",
       "         -2.5825e-01, -1.3504e-01,  2.0688e-01, -1.4902e-01,  4.1835e-02,\n",
       "         -1.0359e-01,  1.8914e-01, -1.2370e-01,  1.4457e-01, -9.4779e-03,\n",
       "          1.4045e-02, -1.9231e-01, -1.4969e-01, -2.3004e-01,  2.9242e-02,\n",
       "          1.1120e-01,  4.9007e-02, -1.0031e-01, -4.4770e-02, -7.7684e-03,\n",
       "          1.0562e-01, -6.4873e-02, -1.4495e-01, -3.4775e-02, -4.4445e-02,\n",
       "          2.4081e-01, -1.6249e-01,  1.1870e-01,  2.6957e-01, -7.2752e-03,\n",
       "         -1.0461e-02,  1.0210e-01,  2.7412e-01, -1.0795e-01,  9.8859e-02,\n",
       "          1.0591e-02, -1.8712e-01,  2.2564e-01,  1.4415e-01, -9.0886e-02,\n",
       "          2.0338e-01, -1.1134e-01, -1.7424e-01,  1.6893e-01],\n",
       "        [ 7.7848e-02, -1.7029e-01, -6.9424e-02, -2.0751e-01, -9.5826e-03,\n",
       "         -6.4059e-02, -1.6965e-02, -1.3191e-01,  1.4035e-01,  9.8969e-02,\n",
       "          5.5259e-02, -3.8584e-02,  1.0935e-01,  1.5136e-01,  1.3059e-01,\n",
       "          9.4317e-02, -1.4280e-02, -9.7914e-02,  4.6483e-02, -8.4167e-02,\n",
       "         -2.6681e-01, -1.3135e-01,  1.1008e-01, -6.8293e-02,  5.6617e-02,\n",
       "         -7.0912e-02,  1.6919e-01, -6.4397e-02,  9.6704e-02, -1.7092e-02,\n",
       "         -4.0824e-02, -1.8465e-01, -2.1080e-01, -2.4400e-01,  4.9413e-02,\n",
       "          3.3591e-02,  1.1291e-02,  3.1249e-02,  7.9186e-04,  4.9106e-02,\n",
       "          7.3069e-02,  8.6718e-03, -1.4520e-01,  8.4769e-03, -4.5103e-02,\n",
       "          2.2780e-01, -1.5380e-01,  3.0211e-02,  2.1982e-01,  2.1138e-02,\n",
       "          3.4632e-02,  1.8506e-01,  2.2594e-01, -8.3130e-02,  7.7641e-02,\n",
       "          2.7274e-02, -1.9055e-01,  2.6331e-01,  1.4821e-01, -9.4643e-02,\n",
       "          6.6107e-02, -1.4115e-01, -8.9209e-02,  1.6111e-01],\n",
       "        [ 9.6450e-02, -1.3590e-01, -3.7022e-02, -1.9476e-01, -6.6803e-02,\n",
       "         -5.8636e-02, -1.6182e-02, -2.2537e-01,  1.7477e-01,  9.6976e-02,\n",
       "          1.1942e-01, -4.8440e-02,  6.0828e-02,  1.1523e-01,  8.2544e-02,\n",
       "          4.6683e-02, -2.0644e-02, -1.2823e-01,  8.8878e-02, -1.1066e-01,\n",
       "         -2.6466e-01, -1.6801e-01,  1.0374e-01, -6.4428e-02, -1.3400e-03,\n",
       "         -1.0604e-01,  2.1548e-01, -1.3059e-01,  1.1448e-01, -3.9414e-02,\n",
       "         -5.1308e-02, -1.7626e-01, -1.2800e-01, -2.2526e-01, -2.4694e-02,\n",
       "          9.0509e-02,  2.6803e-02, -2.2464e-02, -6.9547e-02, -3.6167e-02,\n",
       "          1.6136e-02, -4.7901e-02, -1.4009e-01,  1.8460e-02, -1.6493e-02,\n",
       "          2.7961e-01, -1.1068e-01,  9.1920e-02,  2.0192e-01,  1.1834e-02,\n",
       "         -3.0213e-02,  2.3072e-01,  3.2427e-01, -9.9476e-02,  1.6952e-01,\n",
       "         -9.1901e-03, -1.8155e-01,  2.7055e-01,  8.0764e-02, -6.3265e-02,\n",
       "          1.0073e-01, -5.5032e-02, -8.6618e-02,  1.3735e-01],\n",
       "        [ 4.1030e-02, -1.4613e-01, -8.9348e-03, -1.8106e-01, -3.6819e-02,\n",
       "         -2.7078e-03,  7.4074e-03, -2.3670e-01,  2.0960e-01,  1.3304e-01,\n",
       "          1.1937e-01, -1.2026e-02,  1.5535e-01,  9.8368e-02,  2.6805e-02,\n",
       "          1.2382e-01, -7.9812e-03, -1.4855e-01,  9.6962e-02, -1.1726e-01,\n",
       "         -3.5783e-01, -1.5885e-01,  1.7250e-01, -7.4139e-02,  3.5917e-02,\n",
       "         -1.0061e-01,  1.9002e-01, -6.3256e-02,  7.7965e-02, -7.0127e-02,\n",
       "         -3.6003e-02, -1.7118e-01, -1.8330e-01, -2.2479e-01,  2.3647e-02,\n",
       "          2.9763e-02, -2.3050e-02, -2.9183e-02, -5.0390e-02, -2.1935e-02,\n",
       "          5.6155e-02, -3.2320e-02, -1.8736e-01, -3.6358e-03, -5.1320e-02,\n",
       "          3.1108e-01, -1.6672e-01,  5.3927e-02,  2.5069e-01,  5.5475e-02,\n",
       "         -1.1106e-02,  1.9399e-01,  2.4699e-01, -1.4331e-01,  1.4178e-01,\n",
       "          5.0737e-02, -1.8125e-01,  1.9959e-01,  8.9842e-02, -6.4981e-02,\n",
       "          1.9903e-01, -1.1649e-01, -1.1712e-01,  2.7302e-02],\n",
       "        [ 2.5451e-02, -1.3366e-01, -1.2629e-02, -1.7774e-01, -3.3993e-02,\n",
       "         -2.3133e-02,  3.1457e-02, -2.6208e-01,  1.4327e-01,  8.4041e-02,\n",
       "          1.0913e-01,  6.4816e-03,  1.5178e-01,  5.9311e-02,  4.4070e-02,\n",
       "          6.9249e-02,  6.1193e-03, -1.1849e-01,  1.5167e-01, -1.2023e-01,\n",
       "         -2.9564e-01, -1.8884e-01,  2.2880e-01, -9.3047e-02,  3.9858e-02,\n",
       "         -1.1811e-01,  1.7047e-01,  1.8077e-02,  9.1189e-02, -2.3931e-02,\n",
       "         -5.8258e-02, -1.7732e-01, -1.1516e-01, -2.8632e-01,  1.9114e-02,\n",
       "          8.3748e-02,  7.9152e-02, -3.0150e-02, -7.5312e-02,  2.2068e-02,\n",
       "          9.6147e-02, -4.1757e-02, -8.9386e-02,  5.6880e-02, -1.3732e-02,\n",
       "          2.6056e-01, -1.3944e-01,  7.9034e-02,  2.2835e-01,  1.2503e-02,\n",
       "         -3.3603e-02,  1.8054e-01,  2.7981e-01, -1.4199e-01,  1.3003e-01,\n",
       "          4.3838e-02, -1.9672e-01,  2.2674e-01,  8.4031e-02, -3.7213e-02,\n",
       "          1.7570e-01, -8.4275e-02, -8.0832e-02,  6.8376e-02],\n",
       "        [ 6.5910e-02, -1.1073e-01,  1.9471e-03, -2.3620e-01, -8.3893e-02,\n",
       "         -2.3275e-03,  9.0213e-03, -2.3043e-01,  2.1466e-01,  8.8892e-02,\n",
       "          1.4342e-01,  7.3475e-03,  1.1996e-01,  5.7051e-02,  5.1093e-02,\n",
       "          6.3025e-02, -3.5055e-03, -1.3197e-01,  1.7159e-01, -9.0710e-02,\n",
       "         -2.4548e-01, -1.4955e-01,  1.4681e-01, -1.0020e-01,  2.1666e-02,\n",
       "         -1.0281e-01,  1.6038e-01, -3.0032e-02,  1.4681e-01, -2.5774e-02,\n",
       "         -3.5079e-02, -2.1304e-01, -1.0267e-01, -2.8032e-01, -4.2510e-03,\n",
       "          5.8361e-02,  9.2858e-02, -7.5430e-02, -9.4163e-02, -7.9582e-02,\n",
       "          8.0249e-02, -7.1670e-03, -1.6938e-01, -4.4248e-02, -1.2785e-02,\n",
       "          2.8979e-01, -6.7836e-02,  8.7059e-02,  2.2404e-01,  1.2187e-03,\n",
       "         -6.6876e-02,  2.3852e-01,  2.7576e-01, -9.3841e-02,  1.4384e-01,\n",
       "          8.2912e-03, -1.8627e-01,  2.6667e-01,  3.6114e-02, -6.2181e-03,\n",
       "          1.3817e-01, -7.1339e-02, -1.1867e-01,  7.2230e-02],\n",
       "        [ 5.9836e-02, -1.3812e-01,  5.6671e-02, -2.1128e-01, -3.6429e-02,\n",
       "         -5.1095e-03,  5.5595e-02, -2.2188e-01,  2.1039e-01,  9.9276e-02,\n",
       "          1.0790e-01,  4.9471e-03,  1.2558e-01,  5.5634e-02,  7.9939e-02,\n",
       "          4.4235e-02, -1.2801e-02, -1.2954e-01,  1.3459e-01, -1.0260e-01,\n",
       "         -2.8561e-01, -1.5086e-01,  1.4913e-01, -6.2076e-02,  3.4027e-02,\n",
       "         -1.3119e-01,  1.4384e-01, -6.3936e-02,  8.9322e-02, -2.2668e-02,\n",
       "         -1.5506e-02, -2.0185e-01, -1.0672e-01, -3.1325e-01,  4.5623e-02,\n",
       "          5.1227e-02,  9.9792e-02, -3.2884e-02, -4.6173e-02, -1.2599e-02,\n",
       "          1.5035e-01, -5.1207e-02, -1.1658e-01, -3.2928e-02, -2.1070e-02,\n",
       "          2.7125e-01, -1.3414e-01,  1.1255e-01,  2.1363e-01, -9.2670e-03,\n",
       "         -2.6430e-02,  1.8693e-01,  2.4272e-01, -1.2097e-01,  8.9692e-02,\n",
       "          5.2849e-02, -2.1462e-01,  2.4188e-01,  8.7244e-02, -3.9296e-02,\n",
       "          1.1858e-01, -6.1968e-02, -1.2269e-01,  1.0657e-01],\n",
       "        [ 4.5648e-02, -1.4300e-01,  3.7356e-02, -1.7045e-01, -1.7920e-02,\n",
       "         -6.7778e-02,  4.4820e-02, -1.7424e-01,  1.8532e-01,  5.7462e-02,\n",
       "          1.0954e-01,  2.2376e-02,  8.5535e-02,  1.0629e-01,  8.5688e-02,\n",
       "          4.1423e-02,  3.3088e-03, -4.6214e-02,  8.8318e-02, -9.4122e-02,\n",
       "         -2.7586e-01, -8.4356e-02,  1.0515e-01, -8.0456e-02,  4.2308e-02,\n",
       "         -1.0638e-01,  1.2173e-01, -8.9320e-03,  1.6067e-01,  4.2573e-03,\n",
       "         -4.7365e-02, -1.7514e-01, -1.0523e-01, -3.2577e-01,  3.1915e-02,\n",
       "          2.7145e-02,  4.1258e-02, -3.8895e-03,  2.8937e-03, -5.0953e-03,\n",
       "          9.1287e-02, -3.2419e-02, -1.1814e-01, -4.8815e-02, -4.8639e-02,\n",
       "          2.7411e-01, -1.4108e-01,  4.2194e-02,  1.6510e-01, -2.3429e-04,\n",
       "          6.8069e-02,  2.0235e-01,  2.0427e-01, -3.3003e-02,  4.7273e-02,\n",
       "          6.4293e-02, -2.2994e-01,  3.0385e-01,  8.7954e-02, -4.5069e-02,\n",
       "          4.1001e-02, -8.2138e-02, -1.2104e-01,  1.4463e-01],\n",
       "        [ 6.9173e-02, -1.3596e-01, -5.6916e-02, -1.9959e-01,  5.0025e-02,\n",
       "         -6.9770e-02,  1.9594e-02, -1.9618e-01,  1.9321e-01,  2.0126e-01,\n",
       "          2.9204e-02, -2.6057e-02,  1.3016e-01,  8.1137e-02,  7.6301e-02,\n",
       "          1.0701e-01, -1.5302e-02, -1.5542e-01,  1.9736e-02, -5.8948e-02,\n",
       "         -3.3429e-01, -9.2636e-02,  1.8675e-01, -3.2041e-02,  5.0581e-02,\n",
       "         -7.5106e-02,  8.1995e-02, -2.8587e-02,  1.1405e-01, -1.0643e-02,\n",
       "         -1.2214e-01, -1.7535e-01, -1.5832e-01, -2.2121e-01,  4.9891e-03,\n",
       "          6.2612e-02,  4.4655e-02,  1.1612e-02, -2.1471e-02,  2.7081e-02,\n",
       "          8.3796e-02,  1.2086e-02, -1.9630e-01,  2.3994e-02, -4.1537e-02,\n",
       "          2.9949e-01, -1.5856e-01,  1.8605e-02,  2.4389e-01,  6.0176e-02,\n",
       "          7.4538e-03,  2.7653e-01,  1.7497e-01, -9.1529e-02,  1.4109e-01,\n",
       "          3.4856e-02, -2.4484e-01,  2.1166e-01,  7.8044e-02, -4.7891e-02,\n",
       "          1.7385e-01, -1.6085e-01, -2.7954e-02,  1.4685e-02]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = embed_dim\n",
    "num_heads = num_heads\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "assert (\n",
    "    head_dim * num_heads == embed_dim\n",
    "), \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "dropout = nn.Dropout(p=0)\n",
    "proj_out = nn.Linear(embed_dim, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=64, out_features=192, bias=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = input_tensor\n",
    "B, L, D = x.size()  # Batch size, sequence length, embedding dimension\n",
    "\n",
    "d_model = D\n",
    "wq = nn.Linear(d_model, d_model)\n",
    "wk = nn.Linear(d_model, d_model)\n",
    "wv = nn.Linear(d_model, d_model)\n",
    "# Project queries, keys, and values\n",
    "q = wq(x)\n",
    "k = wk(x)\n",
    "v = wv(x)\n",
    "# qkv = qkv_proj(x)\n",
    "# q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 64])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "q = rearrange(wq(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "k = rearrange(wk(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "v = rearrange(wv(x), 'b n (h d) -> b h n d', h=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = q.view(B, L, num_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10, 10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.einsum('b h i d, b h j d -> b h i j', q, k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (head_dim**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10, 10])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = F.softmax(attn_scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.einsum(\"b h i j, b h j d -> b h i d\", attn_weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10, 8])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 10, 8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_output = torch.matmul(attn_weights, v)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 10, 64])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(attn_output, 'b h n d -> b n (h d)').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: \\ \n",
      "The environment is inconsistent, please check the package plan carefully\n",
      "The following packages are causing the inconsistency:\n",
      "\n",
      "  - defaults/linux-64::mkl==2021.2.0=h06a4308_296\n",
      "  - defaults/linux-64::mkl-service==2.3.0=py38h27cfd23_1\n",
      "  - defaults/linux-64::watchdog==1.0.2=py38h06a4308_1\n",
      "  - defaults/noarch::dask-core==2021.4.0=pyhd3eb1b0_0\n",
      "  - defaults/linux-64::numpy-base==1.20.1=py38h7d8b39e_0\n",
      "  - defaults/noarch::conda-verify==3.4.2=py_1\n",
      "  - defaults/linux-64::gevent==21.1.2=py38h27cfd23_1\n",
      "  - defaults/noarch::jupyter_console==6.4.0=pyhd3eb1b0_0\n",
      "  - defaults/noarch::nbclient==0.5.3=pyhd3eb1b0_0\n",
      "  - defaults/noarch::qtconsole==5.0.3=pyhd3eb1b0_0\n",
      "  - defaults/linux-64::spyder-kernels==1.10.2=py38h06a4308_0\n",
      "  - defaults/linux-64::anaconda-navigator==2.0.3=py38_0\n",
      "  - defaults/noarch::nbclassic==0.2.6=pyhd3eb1b0_0\n",
      "  - defaults/linux-64::spyder==4.2.5=py38h06a4308_0\n",
      "  - defaults/linux-64::widgetsnbextension==3.5.1=py38_0\n",
      "  - defaults/noarch::ipywidgets==7.6.3=pyhd3eb1b0_1\n",
      "  - defaults/linux-64::_ipyw_jlab_nb_ext_conf==0.1.0=py38_0\n",
      "  - defaults/linux-64::jupyter==1.0.0=py38_7\n",
      "  - defaults/linux-64::bokeh==2.3.2=py38h06a4308_0\n",
      "  - defaults/linux-64::conda-build==3.21.4=py38h06a4308_0\n",
      "  - defaults/linux-64::distributed==2021.4.1=py38h06a4308_0\n",
      "  - defaults/linux-64::mkl_fft==1.3.0=py38h42c9631_2\n",
      "  - defaults/linux-64::mkl_random==1.2.1=py38ha9443f7_2\n",
      "  - defaults/noarch::dask==2021.4.0=pyhd3eb1b0_0\n",
      "  - defaults/linux-64::nb_conda_kernels==2.3.1=py38h06a4308_0\n",
      "  - conda-forge/linux-64::boost-cpp==1.70.0=ha2d47e9_1\n",
      "  - conda-forge/linux-64::boost==1.70.0=py38h9de70de_1\n",
      "  - conda-forge/linux-64::rdkit==2019.09.3=py38hb31dc5d_0\n",
      "  - conda-forge/linux-64::pre-commit==2.20.0=py38h578d9bd_0\n",
      "  - conda-forge/linux-64::numba==0.57.1=py38hd559b08_0\n",
      "  - defaults/linux-64::pyarrow==11.0.0=py38h468efa6_1\n",
      "  - conda-forge/linux-64::astropy==5.0.6=py38h07e1bb6_0\n",
      "  - conda-forge/linux-64::scikit-image==0.19.3=py38h8f669ce_2\n",
      "  - conda-forge/noarch::nbformat==5.9.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::huggingface_hub==0.17.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::pyct==0.4.6=py_0\n",
      "  - conda-forge/noarch::colorcet==3.0.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::datasets==2.14.5=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::panel==0.14.4=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::datashader==0.15.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::holoviews==1.17.1=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::transformers==4.33.2=pyhd8ed1ab_0\n",
      "  - conda-forge/noarch::hvplot==0.8.4=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::anaconda-client==1.12.0=pyhd8ed1ab_1\n",
      "  - conda-forge/noarch::anaconda-project==0.11.1=pyhd8ed1ab_0\n",
      "  - defaults/linux-64::conda-repo-cli==1.0.75=py38h06a4308_0\n",
      "  - conda-forge/noarch::intake==0.7.0=pyhd8ed1ab_0\n",
      "  - defaults/linux-64::_anaconda_depends==2023.09=py38_openblas_1\n",
      "  - defaults/linux-64::anaconda==custom=py38_2\n",
      "unsuccessful initial attempt using frozen solve. Retrying with flexible solve.\n",
      "Solving environment: - ^C\n",
      "unsuccessful attempt using repodata from current_repodata.json, retrying with next repodata source.\n",
      "\n",
      "CondaError: KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c pyg pyg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GraphomerBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mmasked_fill(torch\u001b[38;5;241m.\u001b[39meye(num_atoms)\u001b[38;5;241m.\u001b[39mbool(), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m data \u001b[38;5;241m=\u001b[39m Data(x\u001b[38;5;241m=\u001b[39mx, edge_index\u001b[38;5;241m=\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[0;32m---> 19\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[43mGraphomerBlock\u001b[49m(d_model, num_heads, d_ff)\n\u001b[1;32m     20\u001b[0m block(data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GraphomerBlock' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "d_ff = 512\n",
    "num_atoms = 10\n",
    "num_bonds = 20\n",
    "\n",
    "x = torch.randn(num_atoms, d_model)\n",
    "edge_index = torch.randint(0, num_atoms, (2, num_bonds))\n",
    "edge_attr = torch.randn(num_bonds, d_model)\n",
    "mask = torch.ones(num_atoms, num_atoms)\n",
    "mask = mask.masked_fill(torch.eye(num_atoms).bool(), 0)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, mask=mask)\n",
    "\n",
    "block = GraphomerBlock(d_model, num_heads, d_ff)\n",
    "block(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphomer.datamodule import GraphFeaturizer\n",
    "from graphomer.model.embeddings import GraphEmbedding\n",
    "from graphomer.datamodule.featurizer.features import (\n",
    "    ATOM_FEATURES_DIM,\n",
    "    BOND_FEATURES_DIM,\n",
    ")\n",
    "\n",
    "featurizer = GraphFeaturizer()\n",
    "atom_features_dim = list(ATOM_FEATURES_DIM.values())\n",
    "bond_features_dim = list(BOND_FEATURES_DIM.values())\n",
    "graph_encoder = GraphEmbedding(\n",
    "    d_model=128,\n",
    "    atom_features_dim=atom_features_dim,\n",
    "    bond_features_dim=bond_features_dim,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "from graphomer.datamodule.featurizer.featurizer import GraphFeaturizer\n",
    "\n",
    "smi1 = 'c1ccccc1'\n",
    "smi2 = 'c1cccc(CCCCC)c1'\n",
    "feat = GraphFeaturizer()\n",
    "data1 = feat(smi1)\n",
    "data2 = feat(smi2)\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "batch = Batch.from_data_list((data1, data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = to_dense_adj(batch.edge_index, batch.batch, max_num_nodes=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "degree_centrality = torch.sum(adj_matrix, axis=1) / (len(adj_matrix) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 2., 2., 2., 2., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.],\n",
       "        [2., 2., 2., 2., 3., 2., 2., 2., 2., 1., 2., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/db2/users/jungwookim/projects/personal/graphomer_pyg/graphomer/datamodule/featurizer/featurizer.py:96: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403408687/work/torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  x = torch.tensor(atom_features, dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from typing import List, Optional\n",
    "from graphomer.datamodule.featurizer.features import (\n",
    "    ATOM_FEATURES_DIM,\n",
    "    BOND_FEATURES_DIM,\n",
    ")\n",
    "from torch import Tensor\n",
    "from graphomer.model.embeddings import DiscreteEmbedding, ContinuousEmbedding\n",
    "class GraphEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = 768,\n",
    "        atom_features_dim: List[int] = None,\n",
    "        bond_features_dim: List[int] = None,\n",
    "        max_in_degree: int = 512,\n",
    "        max_out_degree: int = 512,\n",
    "        cont_features_dim: Optional[List] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.atom_embedding = DiscreteEmbedding(d_model, atom_features_dim)\n",
    "        self.bond_embedding = DiscreteEmbedding(d_model, bond_features_dim)\n",
    "        # Centrality embedding\n",
    "        self.z_in = DiscreteEmbedding(d_model, [max_in_degree, max_out_degree])\n",
    "        self.z_out = DiscreteEmbedding(d_model, [max_out_degree])\n",
    "\n",
    "        # if cont_features_dim is not None:\n",
    "        #     self.continuous_embedding = ContinuousEmbedding(d_model, cont_features_dim)\n",
    "\n",
    "    def forward(self, data: Data):\n",
    "        print(self.atom_embedding.shape)\n",
    "        data.x = self.atom_embedding(data.x)\n",
    "        data.edge_attr = self.bond_embedding(data.edge_attr)\n",
    "        if self.continuous_embedding:\n",
    "            data.continuous_attr = self.continuous_embedding(data.continuous_attr)\n",
    "        adj_matrix = to_dense_adj(data.edge_index, data.batch)\n",
    "        z_in = self.z_in(adj_matrix.sum(dim=-1))\n",
    "        z_out = self.z_out(adj_matrix.sum(dim=-2))\n",
    "        data.x = data.x + z_in + z_out\n",
    "\n",
    "        return data\n",
    "\n",
    "from torch_geometric.utils import to_dense_adj, to_dense_batch\n",
    "from graphomer.datamodule.featurizer.featurizer import GraphFeaturizer\n",
    "\n",
    "smi1 = \"CC(C)(C)OC(=O)N1CCN(CC2=CC=CC=C2)CC1CCO\"\n",
    "smi2 = \"CC(C)(O)C1=NC(OC2=CC=C3N(C4=CC(C5=CC=CO5)=NC(N)=N4)N=NC3=C2)=CC=C1\"\n",
    "feat = GraphFeaturizer()\n",
    "data1 = feat(smi1)\n",
    "data2 = feat(smi2)\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "batch = Batch.from_data_list((data1, data2))\n",
    "from graphomer.model.embeddings import GraphEmbedding\n",
    "\n",
    "graph_encoder = GraphEmbedding(\n",
    "    atom_features_dim=list(ATOM_FEATURES_DIM.values()), bond_features_dim=list(BOND_FEATURES_DIM.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = graph_encoder(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 32, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_dense_adj(\n",
    "    res.edge_index,\n",
    "    res.batch,\n",
    "    res.edge_attr,\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  4,  5,  0,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "        [ 7, 15,  1,  0,  2,  5,  0,  0,  1,  0,  0],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  0,  0],\n",
       "        [ 7, 15,  1,  0,  1,  5,  0,  0,  1,  0,  0],\n",
       "        [ 6, 14,  1,  0,  3,  5,  0,  0,  1,  0,  1],\n",
       "        [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  1],\n",
       "        [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  1],\n",
       "        [ 6, 14,  1,  0,  3,  5,  0,  0,  2,  0,  1],\n",
       "        [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  1],\n",
       "        [ 5, 13,  1,  0,  4,  5,  1,  0,  2,  0,  1],\n",
       "        [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  0],\n",
       "        [ 7, 15,  1,  0,  2,  5,  1,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  4,  5,  0,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "        [ 7, 15,  1,  0,  2,  5,  1,  0,  2,  0,  0],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 7, 15,  1,  0,  2,  5,  0,  0,  1,  0,  0],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 6, 14,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 7, 15,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "        [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 6, 14,  1,  0,  3,  5,  2,  0,  1,  0,  0],\n",
       "        [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "        [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "        [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "        [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _prepare_graph_input(\n",
    "    self,\n",
    "    x: torch.Tensor,\n",
    "    edge_matrix: torch.Tensor,\n",
    "    hop: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    task_name: Optional[str] = None,\n",
    "):\n",
    "    x = self._encode_node(x)\n",
    "    if self.training and self.perturb_noise != 0.0:\n",
    "        perturb = torch.empty_like(x).uniform_(-self.perturb_noise, self.perturb_noise)\n",
    "        x = x + perturb\n",
    "\n",
    "    # Append Task Token\n",
    "    x_with_task = torch.zeros((x.shape[0], x.shape[1] + 1, x.shape[2]), dtype=x.dtype, device=x.device)\n",
    "    task_token_idx = torch.zeros((x.shape[0],), dtype=torch.long, device=x.device)\n",
    "\n",
    "    x_with_task[:, 1:, :] = x\n",
    "    if task_name is not None and isinstance(self.task_token, nn.ModuleDict):\n",
    "        x_with_task[:, 0, :] = self.task_token[task_name](task_token_idx)\n",
    "    else:\n",
    "        x_with_task[:, 0, :] = self.task_token(task_token_idx)\n",
    "\n",
    "    # Mask with task\n",
    "    if mask is None:\n",
    "        mask_with_task = None\n",
    "    else:\n",
    "        mask_with_task = torch.zeros(\n",
    "            (mask.shape[0], mask.shape[1] + 1),\n",
    "            dtype=mask.dtype,\n",
    "            device=x.device,\n",
    "        )\n",
    "        mask_with_task[:, 1:] = mask\n",
    "        mask_with_task[:, 0] = True\n",
    "\n",
    "    hop_with_task = torch.zeros(\n",
    "        (\n",
    "            hop.shape[0],\n",
    "            hop.shape[1] + 1,\n",
    "            hop.shape[2] + 1,\n",
    "        ),\n",
    "        dtype=hop.dtype,\n",
    "        device=hop.device,\n",
    "    )\n",
    "    # distance with task\n",
    "    # max_hop is $\\mathcal{P}_\\text{far}$\n",
    "    hop_clamped = hop.clamp(max=self.max_hop)\n",
    "    hop_with_task[:, 1:, 1:] = hop_clamped\n",
    "    # extend hop for unreachable # No need\n",
    "    # hop_with_task[:, 0, 1:] = hop_clamped[:, 0, :]   #! need to check\n",
    "    # hop_with_task[:, 1:, 0] = hop_clamped[:, :, 0]   #! need to check\n",
    "    unreachable_mask = hop_with_task == -1\n",
    "    # set task_distance\n",
    "    hop_with_task[:, 0, 1:] = self.TASK_DISTANCE\n",
    "    hop_with_task[:, 1:, 0] = self.TASK_DISTANCE\n",
    "    # set unreachable_distance\n",
    "    hop_with_task[unreachable_mask] = self.UNKNOWN_DISTANCE\n",
    "\n",
    "    # edge matrix with task\n",
    "    edge_matrix_with_task = torch.zeros(\n",
    "        (\n",
    "            edge_matrix.shape[0],\n",
    "            edge_matrix.shape[1] + 1,\n",
    "            edge_matrix.shape[2] + 1,\n",
    "        ),\n",
    "        dtype=edge_matrix.dtype,\n",
    "        device=edge_matrix.device,\n",
    "    )\n",
    "    edge_matrix_with_task[:, 1:, 1:] = edge_matrix\n",
    "    edge_matrix_with_task[hop_with_task != 1] = self.NO_EDGE\n",
    "\n",
    "    # self edge\n",
    "    edge_matrix_with_task[\n",
    "        :,\n",
    "        list(range(edge_matrix_with_task.shape[1])),\n",
    "        list(range(edge_matrix_with_task.shape[2])),\n",
    "    ] = self.SELF_EDGE\n",
    "    edge_matrix_with_task[hop_with_task == self.TASK_DISTANCE] = self.TASK_EDGE\n",
    "    edge_matrix_with_task[unreachable_mask] = self.UNKNOWN_EDGE\n",
    "\n",
    "    return x_with_task, edge_matrix_with_task, hop_with_task, mask_with_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m degree\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdegree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch_geometric/utils/_degree.py:29\u001b[0m, in \u001b[0;36mdegree\u001b[0;34m(index, num_nodes, dtype)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the (unweighted) degree of a given one-dimensional index\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mtensor.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    tensor([3, 1, 1])\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m N \u001b[38;5;241m=\u001b[39m maybe_num_nodes(index, num_nodes)\n\u001b[0;32m---> 29\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((N, ), dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     30\u001b[0m one \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), ), dtype\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mout\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mscatter_add_(\u001b[38;5;241m0\u001b[39m, index, one)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import degree\n",
    "degree(data1.edge_index[0].numpy(), num_nodes = len(data1.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  1,  2,  1,  3,  1,  4,  4,  5,  5,  6,  5,  7,  7,  8,  8,  9,\n",
       "         9, 10, 10, 11, 11, 12, 12, 13, 13, 14, 14, 15, 15, 16, 16, 17, 10, 18,\n",
       "        18, 19, 19, 20, 20, 21, 21, 22, 19,  7, 17, 12])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.edge_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = to_dense_batch(batch.x, batch.batch, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  4,  5,  0,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "          [ 7, 15,  1,  0,  2,  5,  0,  0,  1,  0,  0],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  0,  0],\n",
       "          [ 7, 15,  1,  0,  1,  5,  0,  0,  1,  0,  0],\n",
       "          [ 6, 14,  1,  0,  3,  5,  0,  0,  1,  0,  1],\n",
       "          [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  1],\n",
       "          [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  1],\n",
       "          [ 6, 14,  1,  0,  3,  5,  0,  0,  2,  0,  1],\n",
       "          [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  1],\n",
       "          [ 5, 13,  1,  0,  4,  5,  1,  0,  2,  0,  1],\n",
       "          [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  4,  5,  2,  0,  2,  0,  0],\n",
       "          [ 7, 15,  1,  0,  2,  5,  1,  0,  2,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       " \n",
       "         [[ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  4,  5,  0,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  4,  5,  3,  0,  2,  0,  0],\n",
       "          [ 7, 15,  1,  0,  2,  5,  1,  0,  2,  0,  0],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 7, 15,  1,  0,  2,  5,  0,  0,  1,  0,  0],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 6, 14,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 7, 15,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "          [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 6, 14,  1,  0,  3,  5,  2,  0,  1,  0,  0],\n",
       "          [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "          [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "          [ 6, 14,  1,  0,  2,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  0,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1],\n",
       "          [ 5, 13,  1,  0,  3,  5,  1,  0,  1,  1,  1]]]),\n",
       " tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True, False, False, False, False, False, False, False,\n",
       "          False, False],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphomer.model.graphomer import Graphomer\n",
    "model = Graphomer(d_model=128, num_heads=8, d_ff=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b n (h d) -> b h n d\".\n Input tensor shape: torch.Size([55, 128]). Additional info: {'h': 8}.\n Wrong shape: expected 3 dims. Received 2-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:522\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    521\u001b[0m shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n\u001b[0;32m--> 522\u001b[0m recipe \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_transformation_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_recipe(\n\u001b[1;32m    524\u001b[0m     backend, recipe, cast(Tensor, tensor), reduction_type\u001b[38;5;241m=\u001b[39mreduction, axes_lengths\u001b[38;5;241m=\u001b[39mhashable_axes_lengths\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:365\u001b[0m, in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_names, ndim)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcomposition):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape: expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcomposition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims. Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-dim tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m left_composition \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mcomposition\n",
      "\u001b[0;31mEinopsError\u001b[0m: Wrong shape: expected 3 dims. Received 2-dim tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m dl \u001b[38;5;241m=\u001b[39m DataLoader([data1, data2], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dl:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/db2/users/jungwookim/projects/personal/graphomer_pyg/graphomer/model/graphomer.py:35\u001b[0m, in \u001b[0;36mGraphomer.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/db2/users/jungwookim/projects/personal/graphomer_pyg/graphomer/model/layers.py:20\u001b[0m, in \u001b[0;36mGraphomerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_norm1(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x))\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/db2/users/jungwookim/projects/personal/graphomer_pyg/graphomer/model/layers.py:77\u001b[0m, in \u001b[0;36mMHA.forward\u001b[0;34m(self, q, k, v, attn_bias, mask, need_weights)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     69\u001b[0m     q: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m ):\n\u001b[0;32m---> 77\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb n (h d) -> b h n d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnheads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     k \u001b[38;5;241m=\u001b[39m rearrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk(k), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnheads)\n\u001b[1;32m     79\u001b[0m     v \u001b[38;5;241m=\u001b[39m rearrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv(v), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnheads)\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:591\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Input is list. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b n (h d) -> b h n d\".\n Input tensor shape: torch.Size([55, 128]). Additional info: {'h': 8}.\n Wrong shape: expected 3 dims. Received 2-dim tensor."
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "dl = DataLoader([data1, data2], batch_size=2)\n",
    "for batch in dl:\n",
    "    model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mto_dense_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch_geometric/experimental.py:117\u001b[0m, in \u001b[0;36mdisable_dynamic_shapes.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_experimental_mode_enabled(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_dynamic_shapes\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m required_arg \u001b[38;5;129;01min\u001b[39;00m required_args:\n\u001b[1;32m    120\u001b[0m         index \u001b[38;5;241m=\u001b[39m required_args_pos[required_arg]\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch_geometric/utils/_to_dense_batch.py:97\u001b[0m, in \u001b[0;36mto_dense_batch\u001b[0;34m(x, batch, fill_value, max_num_nodes, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Given a sparse batch of node features\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m:math:`\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}` (with\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m:math:`N_i` indicating the number of nodes in graph :math:`i`), creates a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m            [ True,  True,  True, False]])\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m max_num_nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), mask\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch_geometric/data/data.py:559\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_store\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was created by an older version of PyG. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    556\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf this error occurred while loading an already existing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset, remove the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory in the dataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot folder and try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store, key)\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch_geometric/data/storage.py:96\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[key]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "to_dense_batch(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"b n (h d) -> b h n d\".\n Input tensor shape: torch.Size([55, 128]). Additional info: {'h': 8}.\n Wrong shape: expected 3 dims. Received 2-dim tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:522\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    521\u001b[0m shape \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mshape(tensor)\n\u001b[0;32m--> 522\u001b[0m recipe \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_transformation_recipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _apply_recipe(\n\u001b[1;32m    524\u001b[0m     backend, recipe, cast(Tensor, tensor), reduction_type\u001b[38;5;241m=\u001b[39mreduction, axes_lengths\u001b[38;5;241m=\u001b[39mhashable_axes_lengths\n\u001b[1;32m    525\u001b[0m )\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:365\u001b[0m, in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_names, ndim)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcomposition):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong shape: expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(left\u001b[38;5;241m.\u001b[39mcomposition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dims. Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-dim tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    366\u001b[0m left_composition \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39mcomposition\n",
      "\u001b[0;31mEinopsError\u001b[0m: Wrong shape: expected 3 dims. Received 2-dim tensor.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/db2/users/jungwookim/projects/personal/graphomer_pyg/graphomer/model/graphomer.py:35\u001b[0m, in \u001b[0;36mGraphomer.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mx\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/db2/users/jungwookim/projects/personal/graphomer_pyg/graphomer/model/layers.py:20\u001b[0m, in \u001b[0;36mGraphomerBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mask: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_norm1(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x))\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/db2/users/jungwookim/projects/personal/graphomer_pyg/graphomer/model/layers.py:77\u001b[0m, in \u001b[0;36mMHA.forward\u001b[0;34m(self, q, k, v, attn_bias, mask, need_weights)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     69\u001b[0m     q: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m     need_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m ):\n\u001b[0;32m---> 77\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb n (h d) -> b h n d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnheads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     k \u001b[38;5;241m=\u001b[39m rearrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwk(k), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnheads)\n\u001b[1;32m     79\u001b[0m     v \u001b[38;5;241m=\u001b[39m rearrange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwv(v), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m\"\u001b[39m, h\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnheads)\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:591\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrearrange\u001b[39m(tensor: Union[Tensor, List[Tensor]], pattern: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    537\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m    einops.rearrange is a reader-friendly smart element reordering for multidimensional tensors.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    This operation includes functionality of transpose (axes permutation), reshape (view), squeeze, unsqueeze,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m \n\u001b[1;32m    590\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrearrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43maxes_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/db2/users/jungwookim/miniforge3/envs/graphomer/lib/python3.11/site-packages/einops/einops.py:533\u001b[0m, in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    531\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Input is list. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    532\u001b[0m message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdditional info: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(axes_lengths)\n\u001b[0;32m--> 533\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m EinopsError(message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(e))\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"b n (h d) -> b h n d\".\n Input tensor shape: torch.Size([55, 128]). Additional info: {'h': 8}.\n Wrong shape: expected 3 dims. Received 2-dim tensor."
     ]
    }
   ],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphomer.datamodule.shortest_path import ShortestPathGenerator\n",
    "st = ShortestPathGenerator(max_num_nodes=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'atom_features_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43matom_features_dim\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'atom_features_dim' is not defined"
     ]
    }
   ],
   "source": [
    "atom_features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[23, 11], edge_index=[2, 48], edge_attr=[48, 3], degree=[23, 1], hop=[128, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(st(data2).hop == st(data1).hop).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "g = to_networkx(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "res = nx.all_pairs_shortest_path_length(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 0,\n",
       "  1: 1,\n",
       "  2: 2,\n",
       "  3: 2,\n",
       "  4: 2,\n",
       "  5: 3,\n",
       "  6: 4,\n",
       "  7: 4,\n",
       "  8: 5,\n",
       "  19: 5,\n",
       "  9: 6,\n",
       "  18: 6,\n",
       "  20: 6,\n",
       "  10: 7,\n",
       "  21: 7,\n",
       "  11: 8,\n",
       "  22: 8,\n",
       "  12: 9,\n",
       "  13: 10,\n",
       "  17: 10,\n",
       "  14: 11,\n",
       "  16: 11,\n",
       "  15: 12},\n",
       " 1: {1: 0,\n",
       "  0: 1,\n",
       "  2: 1,\n",
       "  3: 1,\n",
       "  4: 1,\n",
       "  5: 2,\n",
       "  6: 3,\n",
       "  7: 3,\n",
       "  8: 4,\n",
       "  19: 4,\n",
       "  9: 5,\n",
       "  18: 5,\n",
       "  20: 5,\n",
       "  10: 6,\n",
       "  21: 6,\n",
       "  11: 7,\n",
       "  22: 7,\n",
       "  12: 8,\n",
       "  13: 9,\n",
       "  17: 9,\n",
       "  14: 10,\n",
       "  16: 10,\n",
       "  15: 11},\n",
       " 2: {2: 0,\n",
       "  1: 1,\n",
       "  0: 2,\n",
       "  3: 2,\n",
       "  4: 2,\n",
       "  5: 3,\n",
       "  6: 4,\n",
       "  7: 4,\n",
       "  8: 5,\n",
       "  19: 5,\n",
       "  9: 6,\n",
       "  18: 6,\n",
       "  20: 6,\n",
       "  10: 7,\n",
       "  21: 7,\n",
       "  11: 8,\n",
       "  22: 8,\n",
       "  12: 9,\n",
       "  13: 10,\n",
       "  17: 10,\n",
       "  14: 11,\n",
       "  16: 11,\n",
       "  15: 12},\n",
       " 3: {3: 0,\n",
       "  1: 1,\n",
       "  0: 2,\n",
       "  2: 2,\n",
       "  4: 2,\n",
       "  5: 3,\n",
       "  6: 4,\n",
       "  7: 4,\n",
       "  8: 5,\n",
       "  19: 5,\n",
       "  9: 6,\n",
       "  18: 6,\n",
       "  20: 6,\n",
       "  10: 7,\n",
       "  21: 7,\n",
       "  11: 8,\n",
       "  22: 8,\n",
       "  12: 9,\n",
       "  13: 10,\n",
       "  17: 10,\n",
       "  14: 11,\n",
       "  16: 11,\n",
       "  15: 12},\n",
       " 4: {4: 0,\n",
       "  1: 1,\n",
       "  5: 1,\n",
       "  0: 2,\n",
       "  2: 2,\n",
       "  3: 2,\n",
       "  6: 2,\n",
       "  7: 2,\n",
       "  8: 3,\n",
       "  19: 3,\n",
       "  9: 4,\n",
       "  18: 4,\n",
       "  20: 4,\n",
       "  10: 5,\n",
       "  21: 5,\n",
       "  11: 6,\n",
       "  22: 6,\n",
       "  12: 7,\n",
       "  13: 8,\n",
       "  17: 8,\n",
       "  14: 9,\n",
       "  16: 9,\n",
       "  15: 10},\n",
       " 5: {5: 0,\n",
       "  4: 1,\n",
       "  6: 1,\n",
       "  7: 1,\n",
       "  1: 2,\n",
       "  8: 2,\n",
       "  19: 2,\n",
       "  0: 3,\n",
       "  2: 3,\n",
       "  3: 3,\n",
       "  9: 3,\n",
       "  18: 3,\n",
       "  20: 3,\n",
       "  10: 4,\n",
       "  21: 4,\n",
       "  11: 5,\n",
       "  22: 5,\n",
       "  12: 6,\n",
       "  13: 7,\n",
       "  17: 7,\n",
       "  14: 8,\n",
       "  16: 8,\n",
       "  15: 9},\n",
       " 6: {6: 0,\n",
       "  5: 1,\n",
       "  4: 2,\n",
       "  7: 2,\n",
       "  1: 3,\n",
       "  8: 3,\n",
       "  19: 3,\n",
       "  0: 4,\n",
       "  2: 4,\n",
       "  3: 4,\n",
       "  9: 4,\n",
       "  18: 4,\n",
       "  20: 4,\n",
       "  10: 5,\n",
       "  21: 5,\n",
       "  11: 6,\n",
       "  22: 6,\n",
       "  12: 7,\n",
       "  13: 8,\n",
       "  17: 8,\n",
       "  14: 9,\n",
       "  16: 9,\n",
       "  15: 10},\n",
       " 7: {7: 0,\n",
       "  5: 1,\n",
       "  8: 1,\n",
       "  19: 1,\n",
       "  4: 2,\n",
       "  6: 2,\n",
       "  9: 2,\n",
       "  18: 2,\n",
       "  20: 2,\n",
       "  1: 3,\n",
       "  10: 3,\n",
       "  21: 3,\n",
       "  0: 4,\n",
       "  2: 4,\n",
       "  3: 4,\n",
       "  11: 4,\n",
       "  22: 4,\n",
       "  12: 5,\n",
       "  13: 6,\n",
       "  17: 6,\n",
       "  14: 7,\n",
       "  16: 7,\n",
       "  15: 8},\n",
       " 8: {8: 0,\n",
       "  7: 1,\n",
       "  9: 1,\n",
       "  5: 2,\n",
       "  19: 2,\n",
       "  10: 2,\n",
       "  4: 3,\n",
       "  6: 3,\n",
       "  18: 3,\n",
       "  20: 3,\n",
       "  11: 3,\n",
       "  1: 4,\n",
       "  21: 4,\n",
       "  12: 4,\n",
       "  0: 5,\n",
       "  2: 5,\n",
       "  3: 5,\n",
       "  22: 5,\n",
       "  13: 5,\n",
       "  17: 5,\n",
       "  14: 6,\n",
       "  16: 6,\n",
       "  15: 7},\n",
       " 9: {9: 0,\n",
       "  8: 1,\n",
       "  10: 1,\n",
       "  7: 2,\n",
       "  11: 2,\n",
       "  18: 2,\n",
       "  5: 3,\n",
       "  19: 3,\n",
       "  12: 3,\n",
       "  4: 4,\n",
       "  6: 4,\n",
       "  20: 4,\n",
       "  13: 4,\n",
       "  17: 4,\n",
       "  1: 5,\n",
       "  21: 5,\n",
       "  14: 5,\n",
       "  16: 5,\n",
       "  0: 6,\n",
       "  2: 6,\n",
       "  3: 6,\n",
       "  22: 6,\n",
       "  15: 6},\n",
       " 10: {10: 0,\n",
       "  9: 1,\n",
       "  11: 1,\n",
       "  18: 1,\n",
       "  8: 2,\n",
       "  12: 2,\n",
       "  19: 2,\n",
       "  7: 3,\n",
       "  13: 3,\n",
       "  17: 3,\n",
       "  20: 3,\n",
       "  5: 4,\n",
       "  14: 4,\n",
       "  16: 4,\n",
       "  21: 4,\n",
       "  4: 5,\n",
       "  6: 5,\n",
       "  15: 5,\n",
       "  22: 5,\n",
       "  1: 6,\n",
       "  0: 7,\n",
       "  2: 7,\n",
       "  3: 7},\n",
       " 11: {11: 0,\n",
       "  10: 1,\n",
       "  12: 1,\n",
       "  9: 2,\n",
       "  18: 2,\n",
       "  13: 2,\n",
       "  17: 2,\n",
       "  8: 3,\n",
       "  19: 3,\n",
       "  14: 3,\n",
       "  16: 3,\n",
       "  7: 4,\n",
       "  20: 4,\n",
       "  15: 4,\n",
       "  5: 5,\n",
       "  21: 5,\n",
       "  4: 6,\n",
       "  6: 6,\n",
       "  22: 6,\n",
       "  1: 7,\n",
       "  0: 8,\n",
       "  2: 8,\n",
       "  3: 8},\n",
       " 12: {12: 0,\n",
       "  11: 1,\n",
       "  13: 1,\n",
       "  17: 1,\n",
       "  10: 2,\n",
       "  14: 2,\n",
       "  16: 2,\n",
       "  9: 3,\n",
       "  18: 3,\n",
       "  15: 3,\n",
       "  8: 4,\n",
       "  19: 4,\n",
       "  7: 5,\n",
       "  20: 5,\n",
       "  5: 6,\n",
       "  21: 6,\n",
       "  4: 7,\n",
       "  6: 7,\n",
       "  22: 7,\n",
       "  1: 8,\n",
       "  0: 9,\n",
       "  2: 9,\n",
       "  3: 9},\n",
       " 13: {13: 0,\n",
       "  12: 1,\n",
       "  14: 1,\n",
       "  11: 2,\n",
       "  17: 2,\n",
       "  15: 2,\n",
       "  10: 3,\n",
       "  16: 3,\n",
       "  9: 4,\n",
       "  18: 4,\n",
       "  8: 5,\n",
       "  19: 5,\n",
       "  7: 6,\n",
       "  20: 6,\n",
       "  5: 7,\n",
       "  21: 7,\n",
       "  4: 8,\n",
       "  6: 8,\n",
       "  22: 8,\n",
       "  1: 9,\n",
       "  0: 10,\n",
       "  2: 10,\n",
       "  3: 10},\n",
       " 14: {14: 0,\n",
       "  13: 1,\n",
       "  15: 1,\n",
       "  12: 2,\n",
       "  16: 2,\n",
       "  11: 3,\n",
       "  17: 3,\n",
       "  10: 4,\n",
       "  9: 5,\n",
       "  18: 5,\n",
       "  8: 6,\n",
       "  19: 6,\n",
       "  7: 7,\n",
       "  20: 7,\n",
       "  5: 8,\n",
       "  21: 8,\n",
       "  4: 9,\n",
       "  6: 9,\n",
       "  22: 9,\n",
       "  1: 10,\n",
       "  0: 11,\n",
       "  2: 11,\n",
       "  3: 11},\n",
       " 15: {15: 0,\n",
       "  14: 1,\n",
       "  16: 1,\n",
       "  13: 2,\n",
       "  17: 2,\n",
       "  12: 3,\n",
       "  11: 4,\n",
       "  10: 5,\n",
       "  9: 6,\n",
       "  18: 6,\n",
       "  8: 7,\n",
       "  19: 7,\n",
       "  7: 8,\n",
       "  20: 8,\n",
       "  5: 9,\n",
       "  21: 9,\n",
       "  4: 10,\n",
       "  6: 10,\n",
       "  22: 10,\n",
       "  1: 11,\n",
       "  0: 12,\n",
       "  2: 12,\n",
       "  3: 12},\n",
       " 16: {16: 0,\n",
       "  15: 1,\n",
       "  17: 1,\n",
       "  14: 2,\n",
       "  12: 2,\n",
       "  13: 3,\n",
       "  11: 3,\n",
       "  10: 4,\n",
       "  9: 5,\n",
       "  18: 5,\n",
       "  8: 6,\n",
       "  19: 6,\n",
       "  7: 7,\n",
       "  20: 7,\n",
       "  5: 8,\n",
       "  21: 8,\n",
       "  4: 9,\n",
       "  6: 9,\n",
       "  22: 9,\n",
       "  1: 10,\n",
       "  0: 11,\n",
       "  2: 11,\n",
       "  3: 11},\n",
       " 17: {17: 0,\n",
       "  16: 1,\n",
       "  12: 1,\n",
       "  15: 2,\n",
       "  11: 2,\n",
       "  13: 2,\n",
       "  14: 3,\n",
       "  10: 3,\n",
       "  9: 4,\n",
       "  18: 4,\n",
       "  8: 5,\n",
       "  19: 5,\n",
       "  7: 6,\n",
       "  20: 6,\n",
       "  5: 7,\n",
       "  21: 7,\n",
       "  4: 8,\n",
       "  6: 8,\n",
       "  22: 8,\n",
       "  1: 9,\n",
       "  0: 10,\n",
       "  2: 10,\n",
       "  3: 10},\n",
       " 18: {18: 0,\n",
       "  10: 1,\n",
       "  19: 1,\n",
       "  9: 2,\n",
       "  11: 2,\n",
       "  20: 2,\n",
       "  7: 2,\n",
       "  8: 3,\n",
       "  12: 3,\n",
       "  21: 3,\n",
       "  5: 3,\n",
       "  13: 4,\n",
       "  17: 4,\n",
       "  22: 4,\n",
       "  4: 4,\n",
       "  6: 4,\n",
       "  14: 5,\n",
       "  16: 5,\n",
       "  1: 5,\n",
       "  15: 6,\n",
       "  0: 6,\n",
       "  2: 6,\n",
       "  3: 6},\n",
       " 19: {19: 0,\n",
       "  18: 1,\n",
       "  20: 1,\n",
       "  7: 1,\n",
       "  10: 2,\n",
       "  21: 2,\n",
       "  5: 2,\n",
       "  8: 2,\n",
       "  9: 3,\n",
       "  11: 3,\n",
       "  22: 3,\n",
       "  4: 3,\n",
       "  6: 3,\n",
       "  12: 4,\n",
       "  1: 4,\n",
       "  13: 5,\n",
       "  17: 5,\n",
       "  0: 5,\n",
       "  2: 5,\n",
       "  3: 5,\n",
       "  14: 6,\n",
       "  16: 6,\n",
       "  15: 7},\n",
       " 20: {20: 0,\n",
       "  19: 1,\n",
       "  21: 1,\n",
       "  18: 2,\n",
       "  7: 2,\n",
       "  22: 2,\n",
       "  10: 3,\n",
       "  5: 3,\n",
       "  8: 3,\n",
       "  9: 4,\n",
       "  11: 4,\n",
       "  4: 4,\n",
       "  6: 4,\n",
       "  12: 5,\n",
       "  1: 5,\n",
       "  13: 6,\n",
       "  17: 6,\n",
       "  0: 6,\n",
       "  2: 6,\n",
       "  3: 6,\n",
       "  14: 7,\n",
       "  16: 7,\n",
       "  15: 8},\n",
       " 21: {21: 0,\n",
       "  20: 1,\n",
       "  22: 1,\n",
       "  19: 2,\n",
       "  18: 3,\n",
       "  7: 3,\n",
       "  10: 4,\n",
       "  5: 4,\n",
       "  8: 4,\n",
       "  9: 5,\n",
       "  11: 5,\n",
       "  4: 5,\n",
       "  6: 5,\n",
       "  12: 6,\n",
       "  1: 6,\n",
       "  13: 7,\n",
       "  17: 7,\n",
       "  0: 7,\n",
       "  2: 7,\n",
       "  3: 7,\n",
       "  14: 8,\n",
       "  16: 8,\n",
       "  15: 9},\n",
       " 22: {22: 0,\n",
       "  21: 1,\n",
       "  20: 2,\n",
       "  19: 3,\n",
       "  18: 4,\n",
       "  7: 4,\n",
       "  10: 5,\n",
       "  5: 5,\n",
       "  8: 5,\n",
       "  9: 6,\n",
       "  11: 6,\n",
       "  4: 6,\n",
       "  6: 6,\n",
       "  12: 7,\n",
       "  1: 7,\n",
       "  13: 8,\n",
       "  17: 8,\n",
       "  0: 8,\n",
       "  2: 8,\n",
       "  3: 8,\n",
       "  14: 9,\n",
       "  16: 9,\n",
       "  15: 10}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
